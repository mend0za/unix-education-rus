\chapter{IPC. Сокеты. Как писать сетевое ПО.}

Раздел написан по материалам цикла статей "Cетевое программирование в Unix" ( "Сетевые Решения", 2003(12), 2004(1), 2004(2), 2004(3), см http://nestor.minsk.by/sr/abc/114.html ).

\section{Начальные сведения}

\emph{Сокеты (sockets)} впервые появились в начале 80-х в составе 4.2BSD как программный интерфейс к стеку TCP/IP. С тех пор их часто называют "BSD сокеты". И до и после были попытки создавать сетевые API, но сокеты прижились лучше всего. Возможно потому что при их создании учитывались характерные особенности Unix как среды программирования: файла, как основы взаимодействия процессов и принципа "keep it simple stupid" (KISS).

Кстати, в русскоязычной литературе и сленге имеется множество вариантов перевода и произношения термина "socket": сОкет, сокЕт, гнездо (!!!) и т.п. словотворчество. Весь этот зоопарк обозначает одно и то же.

В основе сокетов лежит модная концепция "клиент-сервер". Сервер реализует некоторый сервис, клиент его использует. Два процесса, используя сеть, обмениваются данными. При создании соединения, с каждой из сторон создается сокет. Протоколы TCP/IP отвечают за передачу между ними данных. Чтобы определить кто и куда передает информацию, бы ли введены два понятия: \emph{адрес и порт}. 

\emph{Адрес} - сетевой адрес компьютера, \emph{порт} - это номер сокета на конкретном компьютере. Причем каждый порт связан с конкретным процессом и поэтому операционная система знает кому отдавать пришедшие данные. \emph{Сокет - это пара адрес:порт}.

В итоге - передача данных использует 2 сокета, по одному на каждого участника обмена информацией. Между ними устанавливается виртуальный канал и нет больше об этом заботы в чешуйчатой зеленой голове программиста. Что вписано в сокет на одной стороне, то появится на другой, и наоборот.

\verb+Для иллюстрации: поглядеть 'netstat -natp' (в linux)+

Работа с сокетом идет через файловый дескриптор, что очень характерно для Unix.
Однако есть 2 вида передачи информации: с предварительным соединением и без. Мы подробно прожуем и проглотим оба вида передачи. Так же очень основательно будет рассмотрен серверный и клиентский режимы работы.

\subsection{Всё есть файл}

Работа с сокетом ведется через дескриптор, являющийся также и дескриптором файла. То есть уже открытое соединение ничем не отличается с точки зрения программиста от открытого файла или терминала. Разработчик работает с сокетом теми же методами, которыми он работает с обычным (regular) файлом. Несколько отклоняясь от темы, замечу, что то же самое мы наблюдаем в отношении и других специальных типов файлов (pipe, fifo, block и character devices).

Это подымает термин «файл» на совершенно новый уровень понимания: "мы работаем с этим как с файлом и плевать, что там в реализации". Файл может быть физическим устройством, сокетом, каналом, каталогом, текстовым файлом, но мы только открываем их по разному. Чтение/запись и управление режимами работы - через одни и те же системные вызовы.

Потрясающий по своей красоте и выразительности пример: запись компакт диска по сети в 1 строку.
\begin{verbatim}
$cat 4.9-i386-disc1.iso|ssh user@host.localdomain cdrecord -
\end{verbatim}
Здесь используется сразу многие типы файлов, объединенных командой shell в единое целое.
\verb+cat+ читает файл с диска и выводит на экран. \verb+Оболочка+ перенаправляет вывод \verb+cat+ через \verb+pipe+ на вход команды \verb+ssh+.

\verb+ssh+ запускает на удаленном хосте команду записи дисков. Причем таким образом чтобы cdrecord кушал данные со стандартного ввода.

\verb+ssh+ доставляет ему этот ввод через сокет. Еще один неявный момент - \verb+cdrecord+ использует файл блочного устройства резака для записи полученного образа компакта. Подсчитаем: regular file, pipe, socket, block device - итого 4 различных вида файлов.

Нашему примеру можно придать изящный финальный штрих - избавится от временного файла в лице образа компакта и создавать его на лету:
\begin{verbatim}
$mkisofs /home/user2/for_write|ssh user@host.localdomain cdrecord -
\end{verbatim}

Нет временных файлов, только безостановочное движение потока данных из одного состояния в другое, подкрепленное могучей концепцией "бывают разные файлы, но мы работаем с ними одинаково, просто пишем и читаем".

\subsection{Передача с предварительным соединением и без}

Есть две модели передачи информации посредством сокетов. С установлением соединения и без оного.

Мы в основном будем останавливаться на первой.
\begin{enumerate}
\item Наиболее важные отличия сonnection-oriented соединения (назовем его условно TCP):
\begin{itemize}
\item требует установления соединения перед передачей;
\item гарантирует доставку данных (создание виртуального канала);
\item передача идет потоком данных. TCP/IP гарантирует то что данные дойдут так, как посылались;
\item основан на протоколе TCP.
\end{itemize}
\item connectionless (условно назовем его UDP):
\begin{itemize}
\item не требует соединения;
\item ничего не гарантирует;
\item передача идет пакетами, которые могут пропадать, дублироваться и т.п. Контроль за целостностью информации целиком в руках программиста;
\item основан на протоколе UDP.
\end{itemize}
\end{enumerate}

Итого:

TCP имеет бОльшие накладные расходы чем UDP. Но UDP проще организован, и если нужна высокая скорость и компьютеры стоят в одном сегменте сети, то UDP очень хорошо подойдет. В свою очередь, TCP больше подходит для работы по ненадежным каналам.

Прибегнув к скользкой аналогии,\newline
UDP - выстрел вслепую, пули могут пропасть, а могут и размножится по дороге.\newline
TCP - телефон: что сказано с обеих сторон, то и услышано. Конечно если ворона не села на провод и не перекусила его.

\section{Основные системные вызовы}

Аннотированный список системных вызовов, тем или иным образом качающихся сокетов (в списке есть как socket-specific, так и общие вызовы, связанные с файлами)
\begin{itemize}
\item accept(2) - принять соединение ( используется сервером);
\item bind(2) - связать сокет с конкретным адресом (обычно используется только сервером);
\item connect(2) - соединиться с удаленным сервером (клиентский);
\item close(2) - закрыть файл или сокет;
\item listen(2) - начать прослушивание сокета (серверный);
\item read(2) - чтение данных из файла;
\item recv(2) - чтение из сокета;
\item select(2) - проверка изменения статуса открытых файлов;
\item send(2) - послать данные через сокет;
\item shutdown(2) - закрыть соединение;
\item socket(2) - создать сокет;
\item write(2) - запись данных в файл.
\end{itemize}

\emph{Примечание:} в Solaris, унаследовавшей реализацию сокетов от System V Release 4, сокеты не входят в ядро, а реализованы библиотекой. Поэтому man по ним содержится в 3-м разделе.

\section{Простейший TCP-клиент}

Как ни странно - чаще всего в его роли выступает небезызвестный \verb+telnet+. Утилита \verb+telnet+ есть во всех мне известных реализациях стека tcp/ip. Набираем \verb+telnet <host> <port>+ и наслаждаемся прямым общением с сервером. Благо классические TCP-based протоколы - текстовые.
\begin{verbatim}
[tcpclient.cpp]
1 #include <sys/types.h>
2 #include <sys/socket.h>
3 #include <netinet/in.h>
4 #include <arpa/inet.h>
5 #include <string.h>
6 #include <unistd.h>
7 #include <stdio.h>
8 int main() {
9 struct sockaddr_in addr;
10 memset(&addr,0,sizeof(addr));
11 addr.sin_family=AF_INET;
12 addr.sin_port=htons(1212);
13 addr.sin_addr.s_addr=inet_addr("127.0.0.1");
14 int sock;
15 sock=socket(PF_INET,SOCK_STREAM,0);
16 connect(sock,(sockaddr *)&addr,sizeof(addr));
17 char buf[80+1];
18 memset(buf,0,81);
19 read(sock,buf,80);
20 puts(buf);
21 shutdown(sock,0);
22 close(sock);
23 return 0;
24 }
\end{verbatim}

Эта программка соединяется с localhost, порт 1212, считывает последовательность байт и выводит ее на экран.

А теперь самое вкусное - обгладывание костей, любезно предоставленных нашим информационным спонсором. Строки 1-4: заголовочные файлы. Содержат объявления функций и типов данных для работы через сетевые сокеты. 5 и 7 строки - библиотечные функции Си. 6 строка - заголовочный файл стандартных системных вызовов.

9, 10, 11-13 строки - объявление, зачистка и заполнение структуры, содержащей адрес соединения. Обращаю особое внимание на то, как инициализируется порт и адрес назначения. Адрес и порт хранятся в особом формате, называемом \emph{network byte order}. Прямое присваивание обычно ничего не даст, потому что архитектура i386 и network byte order не совпадают. Макрос \verb+htons(3)+ занимается преобразованием числа в сетевой вид. Функция \verb+inet_addr(3)+ преобразует строковое значение IP-адреса в числовое. \verb+AF_INET+ из 11 строки - указание типа сокетов (сетевые в нашем случае).

И наконец, ключевой момент. 15 строка - создание сокета (TCP-сокета естественно) и 16 строка - соединение с сервером. В \verb+connect(2)+ используется указатель на адрес , потому что размер структуры с адресом может варьироваться в зависимости от типа сокета (сетевой или локальный).

Далее идет считывание (19 строка) и вывод на экран (20 строка) полученного.

21 и 22 строки - закрыть соединение и освободить файловый дескриптор, им использованный.

\section{TCP-сервер}

В качестве примера использована реализация сервиса daytime.
Суть протокола daytime: выдать время и закрыть соединение со стороны сервера.
\begin{verbatim}
[daytime.cpp]
1 #include <sys/types.h>
2 #include <sys/socket.h>
3 #include <netinet/in.h>
4 #include <arpa/inet.h>
5 #include <string.h>
6 #include <time.h>
7 #include <unistd.h>
8 #include <stdio.h>
9 char * daytime() {
10 time_t now;
11 now=time(NULL);
12 return ctime(&now);
13 }
14 int main() {
15 struct sockaddr_in addr;
16 memset(&addr,0,sizeof(addr));
17 addr.sin_family=AF_INET;
18 addr.sin_port=htons(1212);
19 addr.sin_addr.s_addr=INADDR_ANY;//inet_addr("127.0.0.1");
20 int sock, c_sock;
21 sock=socket(PF_INET,SOCK_STREAM,0);
22 bind(sock,(struct sockaddr *)&addr,sizeof(addr));
23 listen(sock,5);
24 for (;;) {
25 c_sock=accept(sock,NULL,NULL);
26 char buf[81];
27 memset(buf,0,81);
28 strncpy(buf,daytime(),80);
29 write(c_sock,buf,strlen(buf));
30 shutdown(c_sock,0);
31 close(c_sock);
32 puts("answer tcp");
33 }
34 return 0;
35 }
\end{verbatim}

Пойдем по порядку. Сервер отличается от клиента режимом работы. Он должен:
\begin{enumerate}
\item занять адрес и порт (22 строка), \verb+bind(2)+;
\item включить прослушивание порта (23 строка), \verb+listen(2)+;
\item принимать и обрабатывать соединения (24-33 строки), \verb+accept(2)+.
\end{enumerate}

Подчеркнем отличия от клиента: в 19 строке указывается специальная константа \verb+INADDR_ANY+. Она обозначает, что соединения будут приниматься на всех интерфейсах. Можно задать и конкретный адрес (в стиле TCP-клиента). Так работают многие сервисы, которым можно задать адрес привязки в конфиге (apache, squid).

С 24 по 33 строку у нас бесконечный цикл. До первого \verb-Ctrl+C-, естественно. Сервер до полной и окончательной победы занимается приемом соединений.

Очень интересная особенность: \emph{каждое входящее соединение порождает свой сокет} (см. 25 строку). Контрольный сокет (\verb+sock+) и клиентский сокет (\verb+c_sock+) - разделены. Обмен данными идет через \verb+c_sock+. Соединения принимаются на контрольный (\verb+sock+).

Есть два важных следствия разделения на контрольный сокет и сокеты соединений:
\begin{itemize}
\item  упрощение работы программиста - нет необходимости отслеживать, кто и куда прислал данные, разные по сути понятия - выделены;
\item возможность распараллеливания обработки соединений (подробнее об этом расскажу позже).
\end{itemize}
В примере используется простая последовательная обработка. Пока предыдущее соединение не обработано - входящие соединения ждут своей очереди. Это имеет смысл только для простых случаев - когда время обработки запроса мало и задержка для вновь поступающих невелика.

9-13 строки - получение текущего времени. Эта функция используется в строке 28 для формирования результата.

\section{UDP}
Вот мы и подошли к финальной части букваря, к протоколу UDP и методам работы с ним.
Начнем традиционно, с клиента, и закончим, как обычно, сервером.
Напоминаю, что UDP - это модель передачи данных без образования соединения и без проверки корректности передачи. Посему если информация пропала - никто не виноват. Но если канал передачи надежный - почему бы и не использовать UDP?
Данные передаются так называемыми датаграммами (datagram) без сохранения порядка при переносе к получателю.

\subsection{UDP-клиент}

К сожалению стандартных и широко распространенных программок типа telnet не обнаружено. Поэтому в этой роли выступает накарябаный на колене за вечер корявый шедевр. Встречайте:
\begin{verbatim}
[udp_client.cpp]
1 #include <sys/types.h>
2 #include <sys/socket.h>
3 #include <netinet/in.h>
4 #include <arpa/inet.h>
5 #include <string.h>
6 #include <unistd.h>
7 #include <stdio.h>

8 int main() {
9 struct sockaddr_in server, client={AF_INET,INADDR_ANY,INADDR_ANY};

10 memset(&server,0,sizeof(server));

11 server.sin_family=AF_INET;
12 server.sin_port=htons(1212);
13 server.sin_addr.s_addr=inet_addr("127.0.0.1");

14 int sock;

15 sock=socket(PF_INET,SOCK_DGRAM,0);
16 bind(sock,(sockaddr *)&client,sizeof(client));

17 char buf[81];
18 memset(buf,0,81);
19 strcpy(buf,"request");
20 sendto(sock,&buf,strlen(buf),0,(sockaddr *)&server,sizeof(server));
21 memset(buf,0,81);
22 recvfrom(sock,buf,80,0,NULL,0);
23 puts(buf);

24 return 0;
25 }
\end{verbatim}

Эта мини-программа посылает слово "request" на порт 1212 по адресу 127.0.0.1 (localhost) и читает, что же ответил сервер.
с 1 по 14 строки - стандартная сокетная обвязка, общая для TCP и UDP. У нас клиент, поэтому в 12 и 13 задаем порт и адрес назначения.

В 15 обратите особое внимание на константу \verb+SOCK_DGRAM+ - мы задаем тип сокета как датаграмный.

16 - уже специфична. Мы явно выполняем привязку адреса и порта к созданному сокету. 9 строка явно указывает, что программисту фиолетово, какой порт занять и через какой из сетевых интерфейсов отсылать данные. Ядро поймет это указание правильно и выдаст порт случайным образом.

20 и 22 строки - следующие специфичные для UDP элементы. Системные вызовы \verb+sendto(2)+ и \verb+recvfrom(2)+ предназначены для отправки и получения сообщений в/из сокета. Их можно использовать даже если сокет находится в несоединенном состоянии. Кстати, несмотря на природу \verb+SOCK_DGRAM+, здесь можно использовать \verb+connect(2)+ и заменить \verb+sendto(2)+ на \verb+write(2)+ и \verb+recvfrom(2)+ на \verb+read(2)+. Будет сделана виртуальная привязка сокета к пункту назначения, но соединение не будет устанавливаться, т.к. сокет остается по прежнему \verb+SOCK_DGRAM+.

\subsection{UDP-сервер}

Опять же daytime сервер в DGRAM-реинкарнации. Ждет датаграмму, при приходе оной извлекает адрес отправителя и посылает в ответ текущее время сервера.
\begin{verbatim}
[udp_server.cpp]

1 #include <sys/types.h>
2 #include <sys/socket.h>
3 #include <netinet/in.h>
4 #include <arpa/inet.h>
5 #include <string.h>
6 #include <time.h>
7 #include <unistd.h>
9 #include <stdio.h>

10 char * daytime() {
11 time_t now;
12 now=time(NULL);
13 return ctime(&now);
14 }

15 int main() {
16 struct sockaddr_in addr;

17 memset(&addr,0,sizeof(addr));

18 addr.sin_family=AF_INET;
19 addr.sin_port=htons(1212);
20 addr.sin_addr.s_addr=INADDR_ANY;

21 int sock, c_sock;

22 sock=socket(PF_INET,SOCK_DGRAM,0);
23 bind(sock,(struct sockaddr *)&addr,sizeof(addr));
24 for (;;) {
25 struct sockaddr from;
26 unsigned int len=sizeof(from);
27 char buf[81];
28 memset(buf,0,81);
29 recvfrom(sock,&buf,80,0,&from,&len);
30 printf("udp incoming:%s",buf);

31 memset(buf,0,81);
32 strncpy(buf,daytime(),80);

33 sendto(sock,buf,strlen(buf),0,&from,len);
34 puts("answer udp");
35 }

36 return 0;
37 }
\end{verbatim}

Все меньше и меньше остается незнакомых элементов в программах. Будь я графоманом и/или любителем гонораров - расписывал бы каждую строку :).

В этом примере мы заострим внимание всего на двух строках - 29 и 33. При вызове \verb+recvfrom(2)+ системным вызовом заполняются параметры 5 и 6. Адрес отправителя и размер структуры для его хранения. Так сервер узнает о существовании клиента и о наличии у него желания пообщаться. И в 33 строке идет подача данных в ответ.

\subsection{Объедки анализа}

Наблюдательные дети наверное заметили, что клиент и сервер у UDP практически идентичны по набору используемых вызовов. Отличие всего в двух маленьких детальках, а дьявол всегда прячется в деталях.

Первая деталька: присваивание адреса и порта сокету: сервер явно указывает порт, клиент - саботирует. Ведь чтобы обратиться к серверу - за ним должен быть зафиксирован порт.

Вторая деталька: порядок вызовов \verb+recvfrom(2)+ и \verb+sendto(2)+. Клиент сначала отсылает, потом принимает, сервер, наоборот, принимает а затем отсылает. Клиент-сервер as is: клиент начинает, сервер реагирует на действия.

\section{Модели организации серверов}

В жизни программиста, пишущего для интерфейса сокетов, через некоторое время неотвратимо наступает переломный момент. Написание приложения, обрабатывающего несколько подключений одновременно, или работающего под серьезной нагрузкой. Или хитрого клиента, который одновременно общается с несколькими серверами.

Существует N-ое количество способов организации нетривиальных серверов и клиентов. Наиболее распространенных из них я и коснусь. Материал будет обильно нашпигован информацией из W. Richard Stevens "Unix. Network Programming. Networking API" и ru.unix.prog FAQ. Поэтому не буду отсылать на конкретные источники, а просто последовательно излагать.

\subsection{"естественный" и "правильный" способы организации сервера}

Чем же руководствоваться при выборе подходящего способа организации сервера? Ответ - здравым смыслом, конечно. Непременно сначала подумать, а не бросаться создавать отстойные приложения.

Первый критерий: подумайте, как и сколько времени обрабатывается 1 соединение.

Если у приложения простой протокол запрос-ответ с минимальной обработкой и задержкой на сервере, простой последовательный сервер будет в самый раз. Клиенты просто не заметят разницы :)

\subsection{Последовательный сервер}

Последовательный - это когда все запросы обрабатываются друг за другом последовательно (см сервис daytime):
\begin{verbatim}
24 for (;;) {
25 c_sock=accept(sock,NULL,NULL);
26 char buf[81];
27 memset(buf,0,81);
28 strncpy(buf,daytime(),80);
29 write(c_sock,buf,strlen(buf));
30 shutdown(c_sock,0);
31 close(c_sock);
32 puts("answer tcp");
33 }
\end{verbatim}

Пока текущий не обработан, следующий запрос на соединение стоит в очереди. Размер очереди указывается в \verb+listen(2)+.
Очень просто и, как ни странно, дьявольски эффективно. Никаких дополнительных накладных расходов на управление соединениями. Но случись задержка в обработке сервером запроса - и остальные клиенты нервно курят в стороне. Длительные операции - увы - не для него.

Как же маштабируемость? Ответ - нуль. Понапихали 4 процессора в машину? Ничего не улучшится :). Все одно, последовательно пилит на одном, ОС бессильна что-либо изменить.
Итого: эффективен, но для простых вещей.

\subsection{один процесс == один клиент (простой и prefork)}

Мысль начинающего сетевого программиста общается сама с собой по очевидной схеме:\newline
- Ага: надо много клиентов одновременно обрабатывать...\newline
- Давайте каждому соединению создадим новый процесс!\newline
- Да, это круто, пельмень!

Так вылазит первая, самая очевидная схема ("естественная"): сервер использует несколько процессов, каждый из которых обслуживает по одному клиенту.

Преимущества этой модели очевидны:
\begin{enumerate}
\item простая как ванильная сушка;
\item хорошо маштабируется с ростом числа процессов;
\item ошибка в 1 процессе не ведет к отказу всей программы.
\end{enumerate}

Но тут есть и отрицательные стороны. Процесс - это достаточно тяжелый объект OC. При большом количестве клиентов мы получаем значительную загрузку системы в целом вплоть до отказа. Переключения контекста и связанная с ними черновая работа ОС вполне могут подсадить на задницу процессор любой мощности (при действительно большом количестве клиентов).

\begin{verbatim}
[fork_server.cpp]

1 for (;;) {
2 c_sock=accept(sock,NULL,NULL);
3 if ( !fork() ) {
4 puts("incoming tcp\n");
5 char time_str[81];
6 memset(time_str,0,81);
7 strncpy(time_str,daytime(),80);
8 write(c_sock,time_str,strlen(time_str));
9 puts("answer tcp\n");
10 shutdown(c_sock,0);
11 close(c_sock);
12 exit(0);
13 }
14 }
\end{verbatim}

Как же это работает? После прихода соединения (строка 2), запускается новый процесс через \verb+fork(2)+ (строка 3). Головной поток выполнения снова вернулся к фазе \verb+accept(2)+, новый же процесс обрабатывает соединение и завершает работу. Почему \verb+fork(2)+ в \verb+if+? Выкурите \verb+man 2 fork+ до полного просветления.

Кстати, приведенный пример - не единственный возможный способ реализации модели «1 процесс == 1 клиент». Существует также улучшенный (и более сложный вариант) организации под названием \emph{prefork}. Суть его следующая: после фазы \verb+listen(2)+ мы запускаем N процессов. В каждом из них - последовательный сервер (см пункт 4.1.1). Вся толпа из N серверов занимается обслуживанием одного и того же принимающего порта. Механизм прост: если одновременно несколько процессов делают \verb+accept(2)+ на одинаковый порт, то ОС погружает всех в спячку до прихода соединения. После прихода соединения - осуществляется т.н. "побудка" и один из процессов побеждает в соц соревновании за сокет. Остальные снова уходят в спячку. Управление этим режимом полностью осуществляется операционной системой.
Плюсы - все уже запущено, только начать обрабатывать. Минус - при количестве процессов >200 начинаются потери в производительности (проверено на людях). Система начинает терять время на обдумывании, кому из 200 процессов отдать на заклание обработку. Как вариант - можно сделать блокировку \verb+accept(2)+ семафором или блокировкой. Тогда только 1 процесс делает \verb+accept(2)+ и разруливание "побудки" не нужно.

\subsection{один поток == один клиент}

Идея проста: меняешь в предыдущем разделе процессы на потоки и защищаешь общие данные. Эффективность подобного решения - величина неизвестная. В разных версиях различных ОС потоки реализованы по разному. Но в общем случае считается, что поток весит меньше чем процесс, и переключение контекста между потоками менее накладно.
\begin{verbatim}
1 for (;;) {
2 pthread_mutex_lock(&mutex_tcp);
3 c_sock=accept(sock,NULL,NULL);
4 pthread_mutex_unlock(&mutex_tcp);
5 pthread_create(&tid,NULL,&deliver_tcp,&c_sock);
6 }
\end{verbatim}

Здесь применена защита \verb+accept(2)+ исключающим семафором (как иллюстрация к идее о prefork, описанной в предыдущем разделе, примененная и к потокам). 2 и 4 строки - блокировка и раз-блокировка мутексом. В строке 5 мы запускаем поток на выполнение функции \verb+deliver_tcp+ с параметром \verb+c_sock+.

\subsection{Однопроцессная Finite State Machine и мультиплексирование}

«Потоки, как и объектно ориентированное программирование - это такая "серебряная пуля". Человеку, которому лень думать над тем, что собственно надлежит сделать, разбиение программы на потоки или использование объектов кажется естественным.
А потом он начинает наступать на не очевидные грабли. Потому что на самом деле это не пуля, а крылатая ракета на жидком водороде. Дальнобойность и убойная сила - поразительные, но обслуживания требует ох какого квалифицированного. И стоит дорого.
Поэтому там, где можно обойтись пулей из автомата (в данном случае - конечного) надо обходиться пулей. А крылатой ракетой стрелять только тогда, когда после анализа других тактических вариантов стало ясно, что здесь ничего другое не поможет.» (Виктор Вагнер, из переписки в ru.unix.рrog.)

В среде специалистов наиболее уважаемым вариантом является реализация c использованием FSM (по русски - на конечном автомате). Конечный автомат - это старая математическая абстракция, описывающая машину с некоторым количеством состояний и переходов между ними.

Ключевая особенность Unix, делающая возможным работу FSM-механизма - средства опроса состояния сокета (через select, poll, kevent/kqueue или epoll).

Сервер опрашивает состояние контрольного сокета и сокетов пришедших соединений. В случае активности на каком-нибудь из них - производит необходимые действия и возвращается в состояние опроса. Если объяснение признано невнятным, вот вам атомный пример.
\begin{verbatim}
Однопоточный TCP и UDP echo-server с использованием опроса состояния сокетов через select. Приведен с сокращениями:

...
1 struct sockaddr_in addr;
/* здесь инициализация addr */

2 int sock, c_sock, u_sock;
/* здесь включение tcp-сервера на sock и udp-сервера на u_sock */

/* определение наборов дескрипторов файлов и их максимального размера */
3 fd_set rfds, afds;
4 int nfds=getdtablesize();
5 FD_ZERO(&afds);
6 FD_SET(sock,&afds);
7 FD_SET(u_sock,&afds);

8 for (;;) {
9 memcpy(&rfds, &afds, sizeof(rfds));
10 select(nfds, &rfds, NULL,NULL,NULL);

11 if ( FD_ISSET(sock, &rfds) ) {
12 c_sock=accept(sock,NULL,NULL);
13 puts("incoming tcp");
14 FD_SET(c_sock,&afds);
15 continue;
16 }

17 for (int fd=0; fd<nfds; ++fd)
18 if ( fd == u_sock && FD_ISSET(u_sock,&rfds) ) {
19 struct sockaddr from;
20 unsigned int len=sizeof(from);
21 char buffer[81];
22 memset(buffer,0,81);
23 int size=recvfrom(u_sock,&buffer,80,0,&from,&len);
24 sendto(u_sock,buffer,size,0,&from,len);
25 printf("answer udp:%s",buffer);

26 } else if ( fd != sock && FD_ISSET(fd,&rfds) ) {
27 char buffer[81];
28 memset(buffer,0,81);
29 int len=read(fd,buffer,80);
30 if (len <=0) {
31 puts("connection closed");
32 close(fd);
33 FD_CLR(fd, &afds);
34 continue;
35 }

36 write(fd,buffer,len);
37 printf("answer tcp:%s",buffer);
38 } // if and for
39 } // for
\end{verbatim}

Echo-сервер делает весьма прозаическую вещь: все, что к нему приходит, отсылается обратно отправителю. Как в tcp, так и в udp. Данная реализация работает одновременно с множественными TCP-соединениями и с присылаемыми UDP-пакетами в одном единственном процессе.

Насладимся же разбором исходников! :) C строки 3 по 7 появляется новый персонаж нашего повествования: набор дескрипторов файлов (\verb+fd_set+). Его зовут на помощь, когда надо оперировать не 1 сокетом за раз, а целой пачкой. Для начала заполним его контрольным TCP- и UDP-сокетом. Это начальное состояние нашей FSM - \verb+afds+, в котором определены два сокета - \verb+sock+ и \verb+u_sock+.

10 строка - вот и опрос состояния сокетов. В начале их у нас два - \verb+sock+ и \verb+u_sock+. По мере выполнения итераций цикла их число может меняться. \verb+select(2)+ изменяет значения \verb+rfds+ - потому в строке 9 мы восстанавливаем его каждый раз из \verb+afds+.

А вот дальше - черная работа. Проверяем, кто из сокетов дернулся:\newline
- sock? Надо принимать соединение и добавлять новый сокет к afds (строки 11-16);\newline
- \verb+u_sock+? Вычитать UDP пакет и вернуть его содержимое отправителю (строки 18-25);\newline
- один из клиентских TCP-сокетов? Прочитать новый кусочек данных и вернуть их назад (строки 26-38).\newline

Теперь немножко о select(2), пожилом, но полным сил мастодонте Unix API.

Параметры 2, 3, 4 - указатели на наборы дескрипторов файлов. Каких файлов - неважно (см. 1 часть цикла "Сетевое программирование в Unix"). Можете одновременно опрашивать stdout, socket и pipe - система с удовольствием схавает и отработает. Соответственно наборы обозначают дескрипторы "доступные для чтения", "доступные для записи", "те, в которых случилась ошибка". Вполне могут отличаться друг от друга, если вам надо сотворить нетривиальную вещь. Часть из них может быть NULL.

Последний, 5-й, параметр - это время, после которого select закончит работать, если ничего не произошло с дескрипторами из наборов. Когда время NULL - ждет вечно, до наступления событий.

И 1-й параметр - это число, равное значению максимального дескриптора из набора +1. Такой параметр выполняет простую функцию - ограничивает количество просматриваемых дескрипторов указанным числом. Чтобы не зацепить лишние, не используемые программой открытые файлы (это замедлит работу). Вобщем select(2) смотрит 0,1,2,3,..n дескрипторы в поисках активности на них. Это то слабое место, в которое обоснованно тыкают пальцем фанаты других способов опроса состояния файла.

Что же можно сказать об этой модели в общем?
Наиболее эффективна с точки зрения использования CPU.
При наличии долгоиграющих блокирующих операций может быть затыки с параллельным приемом других соединений. В этом случае медленные операции можно вынести в отдельные потоки/процессы и решить тем самым затык. Или поработать операционной системой - разбить большую операцию на куски и выполнять ее, попутно отвлекаясь на остальную работу.
Как уже стало очевидно, модель с FSM требует очень тщательного программирования.

\subsection{Смешанные модели}

Вот мы и изучили все базовые типы серверов. Можно заняться их скрещиванием.
Например устойчива к сбоям модель "многопроцессность+FSM". Или производительна (на Linux) "многопоточность+FSM".
Cнова повторюсь. Нет "самой лучшей модели", есть наиболее подходящая к вашим условиям.

